{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring putative circular DNA in parasite and related species\n",
    "\n",
    "This notebook showcases a second use case for the circular DNA-finding workflow. We applied the Nextflow pipeline to a set of parasitic (and related non-parasitic) species to explore potential circular DNAs. This notebook presents the uses of the `GenomeInfo` class to explore annotations and coverage relatively easily.\n",
    "\n",
    "If you'd like to run this, to properly import the `GenomeInfo` class, either add the path to `scripts` into your `$PATH`, or in VSCode change the user setting `Notebook File Root` to `${workspaceFolder}`.\n",
    "\n",
    "This notebook also uses the `parasite_annotation` conda environment.\n",
    "\n",
    "We generated the BAM files and coverage files using the Nextflow pipeline with the following command:\n",
    "\n",
    "```bash\n",
    "nextflow run main.nf --samples inputs/parasite_species_samplesheet.csv --outdir results/parasite_species --threads 16 -resume\n",
    "```\n",
    "\n",
    "Then we moved the BAM files and coverage files into separate subdirectories to make it easier to work with them.\n",
    "\n",
    "I also quickly indexed the BAM files using `samtools index` to make it easier to work with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "import polars as pl\n",
    "\n",
    "# Import the class used to handle the genome information\n",
    "from scripts.genomeinfo import GenomeInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "GFF_DIR = \"inputs/parasite_species/gffs\"\n",
    "BAM_DIR = \"results/parasite_species/bam_files\"\n",
    "COVERAGE_DIR = \"results/parasite_species/coverage_files\"\n",
    "\n",
    "# Get file prefixes from the BAM files\n",
    "file_prefixes = [os.path.basename(f).split(\".large_inserts.bam\")[0] for f in glob.glob(f\"{BAM_DIR}/*.large_inserts.bam\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to load in the required (and optional) files to create a `GenomeInfo` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing GCA_001447435.1_T6_ISS34_r1.0_vs_SRR14041386...\n",
      "Processing GCA_020844145.1_ASM2084414v1_vs_SRR10821146...\n",
      "Processing GCF_013339725.1_ASM1333972v1_vs_SRR14626932...\n",
      "Processing GCF_030504385.1_Musca_domestica.polishedcontigs.V.1.1_vs_SRR650029...\n",
      "Processing GCA_001447585.1_T3_ISS120_r1.0_vs_SRR14041383...\n",
      "Processing GCF_000313135.1_Acastellanii.strNEFF_v1_vs_SRR15928354...\n",
      "Processing GCA_001447565.2_T2_ISS10_r1.1_vs_SRR14041382...\n",
      "Processing GCF_000181795.1_Trichinella_spiralis-3.7.1_vs_SRR14041381...\n",
      "Processing GCA_037355805.1_I.ric_WGA_PrF_JU_vs_SRR18810692...\n",
      "Processing GCF_013339725.1_ASM1333972v1_vs_SRR11263447...\n",
      "Processing GCA_020844145.1_ASM2084414v1_vs_SRR15371646...\n",
      "Processing GCF_000699445.3_UoM_Shae.V3_vs_SRR433860...\n",
      "Processing GCA_013358835.2_BIME_Iper_1.3_vs_SRR25318387...\n",
      "Processing GCF_030504385.1_Musca_domestica.polishedcontigs.V.1.1_vs_SRR4217591...\n",
      "Processing GCF_000648675.2_Clec_2.1_vs_SRR6985627...\n",
      "Processing GCA_000181055.3_Rhodnius_prolixus-3.0.3_vs_SRR6749969...\n",
      "Processing GCF_000648675.2_Clec_2.1_vs_SRR6984045...\n",
      "Processing GCA_001447645.1_T4_ISS176_r1.0_vs_SRR6976817...\n",
      "Processing GCA_001447755.1_T10_ISS1980_r1.0_vs_SRR1971673...\n",
      "Processing GCA_002221485.1_ASM222148v1_vs_SRR14041385...\n",
      "Processing GCA_001447505.1_T9_ISS409_r1.0_vs_SRR14041389...\n",
      "Processing GCA_013358835.2_BIME_Iper_1.3_vs_SRR25318385...\n",
      "Processing GCA_001447745.1_T8_ISS272_r1.0_vs_SRR14041388...\n",
      "Processing GCA_001447665.1_T11_ISS1029_r1.0_vs_SRR14041391...\n",
      "Processing GCF_013339725.1_ASM1333972v1_vs_SRR14626935...\n",
      "Processing GCA_037355805.1_I.ric_WGA_PrF_JU_vs_SRR18810694...\n",
      "Processing GCF_003426905.1_ASM342690v1_vs_SRR13621574...\n",
      "Processing GCA_942486065.2_idCalVomi1.2_vs_SRR11948541...\n",
      "Processing GCA_000469725.3_EMULTI002_vs_SRR22558380...\n",
      "Processing GCF_013339725.1_ASM1333972v1_vs_SRR25318347...\n",
      "Processing GCF_000699445.3_UoM_Shae.V3_vs_SRR433863...\n",
      "Processing GCA_001447455.1_T7_ISS37_r1.0_vs_SRR14041387...\n",
      "Processing GCA_013358835.2_BIME_Iper_1.3_vs_SRR25318384...\n",
      "Processing GCA_020844145.1_ASM2084414v1_vs_SRR15371659...\n",
      "Processing GCA_001447645.1_T4_ISS176_r1.0_vs_SRR14041384...\n",
      "Processing GCA_000181055.3_Rhodnius_prolixus-3.0.3_vs_SRR6749978...\n",
      "Processing GCF_013339725.1_ASM1333972v1_vs_SRR25318334...\n",
      "Processing GCA_001447655.1_T12_ISS2496_r1.0_vs_SRR14041392...\n",
      "Processing GCF_000648675.2_Clec_2.1_vs_SRR1660254...\n",
      "Processing GCF_030504385.1_Musca_domestica.polishedcontigs.V.1.1_vs_SRR650116...\n",
      "Processing GCF_013339725.1_ASM1333972v1_vs_SRR25318329...\n",
      "Processing GCA_000181055.3_Rhodnius_prolixus-3.0.3_vs_SRR15276518...\n",
      "Processing GCA_001447755.1_T10_ISS1980_r1.0_vs_SRR14041390...\n",
      "Processing GCA_000469725.3_EMULTI002_vs_SRR23725278...\n",
      "Processing GCA_037355805.1_I.ric_WGA_PrF_JU_vs_SRR18810686...\n",
      "Processing GCF_016920785.2_ASM1692078v2_vs_SRR13398845...\n",
      "Processing GCA_000181055.3_Rhodnius_prolixus-3.0.3_vs_SRR15276519...\n",
      "Processing GCF_016920785.2_ASM1692078v2_vs_SRR13398844...\n"
     ]
    }
   ],
   "source": [
    "genome_infos = []\n",
    "\n",
    "for prefix in file_prefixes:\n",
    "    print(f\"Processing {prefix}...\")\n",
    "    genome_info = GenomeInfo(\n",
    "        large_insert_bam_loc = f\"{BAM_DIR}/{prefix}.large_inserts.bam\",\n",
    "        large_insert_bai_loc = f\"{BAM_DIR}/{prefix}.large_inserts.bam.bai\",\n",
    "        coverage_loc = f\"{COVERAGE_DIR}/{prefix}.filtered_coverage.txt\",\n",
    "        gff_loc = f\"{GFF_DIR}/{prefix.split('_vs_')[0]}_genomic.gff.gz\",\n",
    "        genome_metadata = {\"prefix\": prefix}\n",
    "    )\n",
    "    genome_info.load_bam()\n",
    "    genome_info.load_coverage()\n",
    "    if os.path.exists(genome_info.gff_loc):\n",
    "        genome_info.load_gff()\n",
    "    genome_infos.append(genome_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create and write the analyzed files to the `results/parasite_species/insert_filtered_results/` directory. This step creates the following files for each genome/SRA pair:\n",
    "1. [genome]_vs_[sra_accession]_deduped_insert_summary.csv: the output of the insert size analysis, with the genomic ranges and counts for each insert size with a z score over the threshold (default 5 in this case). It also is filtered to only include insert sizes <= 100kb.\n",
    "2. [genome]_vs_[sra_accession]_insert_filtered_coverage.csv: the coverage data filtered to only include the insert sizes found in the insert summary file.\n",
    "3. [genome]_vs_[sra_accession]_insert_filtered_gff.csv: the annotation data filtered to only include the insert sizes found in the insert summary file.\n",
    "4. [genome]_vs_[sra_accession]_merged_bam_coverage.csv: the merged coverage and insert size data produced after finding outlier inserts and positions.\n",
    "\n",
    "The GFF outputs may or may not exist depending on the genome, as not all genomes have annotations available.\n",
    "\n",
    "This is just one way to use `GenomeInfo`: if you want to tweak the parameters or look at the insert data without adding genomic ranges around them, please do so!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genome_params = {\"maximum_insert_size_threshold\": 100000,\n",
    "                 \"genomic_range_width_bp\": 10000,\n",
    "                 \"z_score_threshold\": 5}\n",
    "\n",
    "peak_dict = {}\n",
    "\n",
    "# Check if the output directory exists, if not create it\n",
    "OUTPUT_DIR = \"results/parasite_species/insert_filtered_results\"\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "\n",
    "for genome in genome_infos:\n",
    "    print(f\"Starting work on {genome.genome_metadata['prefix']}\")\n",
    "    peak_dict[genome.genome_metadata[\"prefix\"]] = {}\n",
    "    # Generate the deduplicated insert summary for the genome and reads\n",
    "    deduped_output = f\"{OUTPUT_DIR}/{genome.genome_metadata['prefix']}_deduped_insert_summary.csv\"\n",
    "    if not os.path.exists(deduped_output):\n",
    "        insert_summary = genome.generate_insert_summary(maximum_size_threshold=genome_params[\"maximum_insert_size_threshold\"])\n",
    "        peak_dict[genome.genome_metadata[\"prefix\"]][\"insert_summary\"] = insert_summary\n",
    "        insert_range = genome.generate_insert_range(insert_summary, width_bp=genome_params[\"genomic_range_width_bp\"])\n",
    "        peak_dict[genome.genome_metadata[\"prefix\"]][\"insert_range\"] = insert_range\n",
    "        deduped = genome.deduplicate_insert_summary(insert_range, z_score_threshold=genome_params[\"z_score_threshold\"])\n",
    "        peak_dict[genome.genome_metadata[\"prefix\"]][\"deduped\"] = deduped\n",
    "        deduped.write_csv(\n",
    "            f\"{OUTPUT_DIR}/{genome.genome_metadata['prefix']}_deduped_insert_summary.csv\"\n",
    "        )\n",
    "    else:\n",
    "        deduped = pl.read_csv(deduped_output)\n",
    "        peak_dict[genome.genome_metadata[\"prefix\"]][\"deduped\"] = deduped\n",
    "\n",
    "    # Filter the coverage file to only positions within detected mapped distance sizes\n",
    "    filtering_pos = genome.generate_filtering_positions(deduped)\n",
    "    \n",
    "    cov_output = f\"{OUTPUT_DIR}/{genome.genome_metadata['prefix']}_insert_filtered_coverage.csv\"\n",
    "    if not os.path.exists(cov_output):\n",
    "        filtered_coverage = genome.filter_coverage(filtering_pos)\n",
    "        peak_dict[genome.genome_metadata[\"prefix\"]][\"filtering_pos\"] = filtering_pos\n",
    "        filtered_coverage.write_csv(\n",
    "            f\"{OUTPUT_DIR}/{genome.genome_metadata['prefix']}_insert_filtered_coverage.csv\"\n",
    "        )\n",
    "        peak_dict[genome.genome_metadata[\"prefix\"]][\"coverage\"] = filtered_coverage\n",
    "    else:\n",
    "        # Check if filtering_pos is empty, if not load file\n",
    "        if len(filtering_pos) >= 1:\n",
    "            filtered_coverage = pl.read_csv(cov_output)\n",
    "            peak_dict[genome.genome_metadata[\"prefix\"]][\"filtering_pos\"] = filtering_pos\n",
    "            peak_dict[genome.genome_metadata[\"prefix\"]][\"coverage\"] = filtered_coverage\n",
    "\n",
    "    # If there's a GFF file, filter it to only include positions within detected mapped distance sizes\n",
    "    if genome.gff is not None:\n",
    "        gff_output = f\"{OUTPUT_DIR}/{genome.genome_metadata['prefix']}_insert_filtered_gff.csv\"\n",
    "        if not os.path.exists(gff_output):\n",
    "            filtered_gff = genome.filter_gff(filtering_pos)\n",
    "            peak_dict[genome.genome_metadata[\"prefix\"]][\"filtered_gff\"] = filtered_gff\n",
    "            # Check if the filtered gff is empty, if not write it out\n",
    "            if not filtered_gff.is_empty():\n",
    "                filtered_gff.write_csv(\n",
    "                    f\"{OUTPUT_DIR}/{genome.genome_metadata['prefix']}_insert_filtered_gff.csv\"\n",
    "                )\n",
    "        else:\n",
    "            filtered_gff = pl.read_csv(gff_output, ignore_errors=True)\n",
    "            peak_dict[genome.genome_metadata[\"prefix\"]][\"filtered_gff\"] = filtered_gff\n",
    "\n",
    "    # Filter the BAM file and merge together with the filtered coverage, only if the filtering_pos is not empty\n",
    "    # If genome prefix has \"Musca\", skip - too large to process in a reasonable amount of time\n",
    "    if len(filtering_pos) >= 1:\n",
    "        if \"Musca\" not in genome.genome_metadata[\"prefix\"]:\n",
    "            bam_cov_output = f\"{OUTPUT_DIR}/{genome.genome_metadata['prefix']}_merged_bam_coverage.csv\"\n",
    "            if not os.path.exists(bam_cov_output):\n",
    "                bam_data = []\n",
    "                for read in genome.bam.fetch():\n",
    "                    # Check if read.reference_name is in filtering_pos keys, skip if not\n",
    "                    if read.reference_name not in filtering_pos.keys():\n",
    "                        continue\n",
    "                    else:\n",
    "                        chr_filtering_pos = filtering_pos[read.reference_name]\n",
    "                        if read.reference_start in chr_filtering_pos:\n",
    "                            appendable_data = {\n",
    "                                \"query_name\": read.query_name,\n",
    "                                \"chromosome\": read.reference_name,\n",
    "                                \"position\": read.reference_start,\n",
    "                                \"reference_end\": read.reference_end,\n",
    "                                \"length\": read.template_length,\n",
    "                                \"cigar\": read.cigarstring,\n",
    "                            }\n",
    "                            bam_data.append(appendable_data)\n",
    "                bam_df = pl.DataFrame(bam_data)\n",
    "                # Check if filtered_coverage is empty, if not merge it with the bam_df\n",
    "                if not filtered_coverage.is_empty():\n",
    "                    merged_df = bam_df.join(filtered_coverage, on=[\"chromosome\", \"position\"])\n",
    "                merged_df.write_csv(\n",
    "                        f\"{OUTPUT_DIR}/{genome.genome_metadata['prefix']}_merged_bam_coverage.csv\"\n",
    "                    )\n",
    "            else:\n",
    "                merged_df = pl.read_csv(bam_cov_output)\n",
    "                peak_dict[genome.genome_metadata[\"prefix\"]][\"merged_bam_coverage\"] = merged_df\n",
    "    \n",
    "    print(f\"Finished working on {genome.genome_metadata['prefix']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genomeinfo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
